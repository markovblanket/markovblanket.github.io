<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#943526">

	
	<meta property="og:image" content="https://markovblanket.github.io/assets/rejection.png">
	

	
	<meta property="og:title" content="Probabilistic Machine Learning â€¢ Variational Inference">
	

	<link rel="icon" href="/favicon.png" type="image/png">

	<title>Variational Inference</title>
	<meta name="description" content="In probabilistic machine learning we are always assuming that our observations  are samples generated from probability distributions and we are dealing with ...">

	<link rel='stylesheet' id='libretto-fonts-css'  href='https://fonts.googleapis.com/css?family=Libre+Baskerville%3A400%2C700%2C400italic%7CPlayfair+Display%3A400%2C700%2C400italic%2C700italic%7CPlayfair+Display+SC%3A700%2C700italic%7CMontserrat%3A400%7CDroid+Sans+Mono%3A400&#038;subset=latin%2Clatin-ext' type='text/css' media='all'>

	<link rel="stylesheet" href="/css/main.css">
	<link rel="canonical" href="https://markovblanket.github.io/2018/12/28/variational-inference.html">
	<link rel="alternate" type="application/rss+xml" title="Probabilistic Machine Learning" href="https://markovblanket.github.io/feed.xml">

	<script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
	      jax: ["input/TeX", "output/HTML-CSS"],
	      tex2jax: {
	        inlineMath: [ ['$', '$'], ["\(", "\)"] ],
	        displayMath: [ ['$$', '$$'], ["\[", "\]"] ],
	        processEscapes: true,
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
	      }
	      //,
	      //displayAlign: "left",
	      //displayIndent: "2em"
	    });
	</script>
	<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>



</head>


<body>

	<header class="site-header">

	<div class="wrapper">

		<a class="site-title" href="/">Probabilistic Machine Learning</a>

	</div>

</header>


	<div class="page-content">
		<div class="wrapper">
			<header class="post-header">
	<div>
		<span>Posted on </span><span class="post-meta">December 28, 2018</span>
	</div>

	<h1 class="post-title" itemprop="name headline">Variational Inference</h1>
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="entry-content" itemprop="articleBody">
		<p>In probabilistic machine learning we are always assuming that our observations <script type="math/tex">\{x_1,x_2,\ldots,x_N\}</script> are samples generated from probability distributions and we are dealing with their joint probability:</p>

<script type="math/tex; mode=display">\begin{equation}
p(x[0],x[1],\ldots,x[N-1];\theta)
\end{equation}</script>

<p>For each particular problem 
(or dataset<script type="math/tex"></script>)
 we can define our model for this joint probability. 
For example when the probability distribution has semicolon (like <script type="math/tex">p(x;\theta)</script>), it means that in our predefined model we have a deterministic but unknwon parameter <script type="math/tex">\theta</script> 
<!-- that by using an approach such as maximum likelihood this parameter (or a set of parameters$$$$) can be found. -->
and we are looking for an estimator (a function of samples<script type="math/tex"></script>)
to
 approximate this parameter
:</p>

<script type="math/tex; mode=display">\hat{\theta}=g(x[0],x[1],\ldots,x[N-1])</script>

<p>For example in 
language modeling we can assume words in a sentence are i.i.d data samples  (the sentence is an array of these words <script type="math/tex">s=\{w[0],w[1],\ldots,w[N-1] \}</script>) and our aim is to find the parameters of their distribution:</p>

<script type="math/tex; mode=display">w[i]\sim p(w;\theta)</script>

<p>We can improve this model by adding latent variables.  In language modeling we know that sentences are generated by grammar rules, so this prior information can help us to make more accurate models. In other words, in this new model first there are samples generated from hidden variables (<script type="math/tex">\mathbf{z}</script>) and then our observations (<script type="math/tex">\mathbf{x}</script>) are generated by them.</p>

<p align="center">
  <img width="8%" height="8%" src="/assets/xz.png" />
</p>
<p>After introducing the latent variables to our model instead of the joint probability of datasamples we should define the joint probability of data and the hidden variables in <script type="math/tex">p_{\boldsymbol{\theta}}(\mathbf{x},\mathbf{z})</script>.</p>

<p>A good example for adding latent variables is clustering. Suppose that first we just see the raw datapoints:</p>
<p align="center">
  <img width="60%" height="60%" src="/assets/2_clusters_data.png" />
</p>

<p>After observing the data I can decide to have two clusters in my model and simply define
 <script type="math/tex">p_{\boldsymbol{\theta}}(\mathbf{x}|z=0)
 	=
 	\mathcal{N}(\mu_0,\sigma_0)</script>
and 
similarly
 <script type="math/tex">p_{\boldsymbol{\theta}}(\mathbf{x}|z=1)
 	=
 	\mathcal{N}(\mu_1,\sigma_1)</script>
.
 Also I should define the prior information for <script type="math/tex">p_{\boldsymbol\theta}(\mathbf{z})</script>. (We sometimes assume that the prior for <script type="math/tex">\mathbf{z}</script> lacks any parameter such as <script type="math/tex">\mathcal{N}(0,1)</script>). So in this clustering example our model for the conditional probability is Gaussian and the parameters for them are <script type="math/tex">\boldsymbol{\theta}=\{\mu_0,\sigma_0,\mu_1,\sigma_1\}</script>.</p>

<p align="center">
  <img width="60%" height="60%" src="/assets/2_clusters_z_2.png" />
</p>

<p>Now since <script type="math/tex">\mathbf{z}</script> is not deterministic and it was defined to be a random variable, after observing the data we want to infer <script type="math/tex">p(\mathbf{z}|\mathbf{x})</script> 
 and we call it the <em>posterior</em>.
By using Bayes rule we can define this distribution 
based on the model we defined:</p>

<script type="math/tex; mode=display">p(\mathbf{z}|\mathbf{x})=\dfrac{p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})}{p_{\boldsymbol\theta}(\mathbf{x})}</script>

<p>Also we know that in continuous case:</p>

<script type="math/tex; mode=display">\begin{equation}
p_{\boldsymbol\theta}(\mathbf{x})=\int 
p_{\boldsymbol\theta}(\mathbf{x}|\mathbf{z})
p_{\boldsymbol\theta}(\mathbf{z})
 \, \mathrm{d}\mathbf{z}
\end{equation}</script>

<p>Since this integral is over all possible values for <script type="math/tex">\mathbf{z}</script> if the dimension of <script type="math/tex">\mathbf{z}</script> is high or the number of different possible values it can take is infinite then 
<script type="math/tex">p(\mathbf{z}|\mathbf{x})</script> 
is intractable. Now one of the solutions would be estimating 
<script type="math/tex">p(\mathbf{z}|\mathbf{x})</script> by defining a tractable distribution 
(
a simple distribution that we define ourselves<script type="math/tex"></script>)
such as <script type="math/tex">q_{\mathbf{\phi}}(\mathbf{z})</script> and minimizing the divergence between <script type="math/tex">q_{\boldsymbol{\phi}}(\mathbf{z})</script> and <script type="math/tex">p(\mathbf{z}|\mathbf{x})</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}

D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)&=
\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg [ \log\frac{
	q_\boldsymbol{\phi}({\mathbf{z}})
}{
	p(\mathbf{z}|\mathbf{x})
}\bigg ] \\
&=\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg [ \log\frac{
	q_\boldsymbol{\phi}({\mathbf{z}})p_{\boldsymbol\theta}(\mathbf{x})
}{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}\bigg] \\

&=\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg [ \log\frac{
	q_\boldsymbol{\phi}({\mathbf{z}})
}{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}\bigg]
+
\mathbb{E}_{q_\boldsymbol{\phi}(\mathbf{z})}
\log p_{\boldsymbol\theta}{(\mathbf{x})}

 \\

&=\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg[ \log\frac{
	q_\boldsymbol{\phi}({\mathbf{z}})
}{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}\bigg]+\log p_{\boldsymbol\theta}(\mathbf{x}) \\

\end{align} %]]></script>

<p>We have used the equation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\mathbb{E}_{q_{\boldsymbol{\phi}}{(\mathbf{z})}}\big [\log p_{\boldsymbol\theta}(\mathbf{x}) \big]&=\int q_{\boldsymbol{\phi}}(\mathbf{z})
	\log p_{\boldsymbol\theta}(\mathbf{x})\,\mathrm{d}\mathbf{z} \\
	&=\log p_{\boldsymbol\theta}(\mathbf{x})
	\end{align} %]]></script>

<p>Finally we can write:</p>

<script type="math/tex; mode=display">\underbrace{
\log p_{\boldsymbol\theta}(\mathbf{x})
}_{\text{evidence}}
=
D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)
+
\underbrace{
\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg[ \log\frac{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}{
	q_\boldsymbol{\phi}({\mathbf{z}})
}\bigg]
}_{\text{ELBO}}</script>

<p>We should note than the set of <script type="math/tex">\{
\boldsymbol\theta,\boldsymbol\phi
\}</script> are the parameters we want to estimate
. As we see the evidence is just a function of <script type="math/tex">\boldsymbol\theta</script> so minimizing <script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)</script> with respect to <script type="math/tex">\boldsymbol\phi</script> is equal to maximizing the ELBO with respect to this parameter. Also maximizing the ELBO with respect to <script type="math/tex">\boldsymbol\theta</script> is equal to maximizing the likelihood. So instead of dealing with 
<script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)</script> which is intractable, we  use the ELBO as our objective function.</p>

<h1 id="why-did-we-start-with-d_mathrmklbigq_boldsymbolphimathbfzpmathbfzmathbfxbig">Why did we start with <script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)</script>?</h1>

<p>In variational inference we are approximating an intractable distribution with a tractable one and we want to minimize their divergence (which is different from distance<script type="math/tex"></script>). KL divergence has some important properties:</p>

<p><strong>1-</strong> <script type="math/tex">D_{\mathrm{KL}}(P\|Q)\geq 0
\quad \forall P,Q</script></p>

<p><strong>2-</strong> <script type="math/tex">D_{\mathrm{KL}}(P\|Q)=0
\quad</script> if and only if <script type="math/tex">P=Q</script></p>

<p>So based on these two conditions we see that both <script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)</script> and 
<script type="math/tex">D_{\mathrm{KL}}\big(p(\mathbf{z}|\mathbf{x})\|
q_{\boldsymbol\phi}(\mathbf{z})
\big)</script>
 could be our objective functions to make <script type="math/tex">p(\mathbf{z}|\mathbf{x})</script> and <script type="math/tex">q_{\boldsymbol\phi}(\mathbf{z})</script> as similar as possible but the problem with <script type="math/tex">D_{\mathrm{KL}}\big(p(\mathbf{z}|\mathbf{x})\|
q_{\boldsymbol\phi}(\mathbf{z})
\big)</script> is the intractable part:</p>

<script type="math/tex; mode=display">D_{\mathrm{KL}}\big(p(\mathbf{z}|\mathbf{x})\|
q_{\boldsymbol\phi}(\mathbf{z})
\big)=
\int_{\mathbf{z}}{
\underbrace{
\dfrac{p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})}{p_{\boldsymbol\theta}(\mathbf{x})}
}
_{f(p)}
}
\log{
\big(
\underbrace{
\dfrac{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}{q_{\boldsymbol\phi}(\mathbf{z})p_{\boldsymbol\theta}(\mathbf{x})}}
_{g(p,q)}
\big)
}
\, \mathrm{d}{\mathbf{z}}</script>

<p>We should look for a way to separate tractable and intractable parts from <script type="math/tex">p(\mathbf{z}|\mathbf{x};\boldsymbol{\theta})</script> and since both <script type="math/tex">f(p)</script> and <script type="math/tex">g(p,q)</script> include <script type="math/tex">p_{\boldsymbol\theta}(\mathbf{x})</script>, itâ€™s not possible to derive tractable parts from the integral but as we showed in <script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p(\mathbf{z}|\mathbf{x})\big)</script> the intractable part (<script type="math/tex">p_{\boldsymbol\theta}(x)</script>) is just inside the <script type="math/tex">\log</script> function which leads to ELBO.</p>
<h1 id="what-does-elbo-mean">What does ELBO mean?</h1>

<p>By looking at the formula derived for the ELBO, we see:</p>

<script type="math/tex; mode=display">\begin{align}
\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}\bigg[ \log\frac{
	p_{\boldsymbol{\theta}}(\mathbf{x}|\mathbf{z})p_{\boldsymbol\theta}(\mathbf{z})
}{
	q_\boldsymbol{\phi}({\mathbf{z}})
}\bigg]=
\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}
{\log p_{\boldsymbol\theta}(\mathbf{x}|\mathbf{z})}
-
D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p_{\boldsymbol\theta}(\mathbf{z})\big)
\end{align}</script>

<p>The ELBO shows that when we are approximating 
 the posterior with <script type="math/tex">q_{\boldsymbol\phi}(\mathbf{z})</script>, first we have some prior information about <script type="math/tex">p_{\theta}(\mathbf{z})</script> and it makes sense to minimize 
 <script type="math/tex">D_{\mathrm{KL}}\big(q_{\boldsymbol\phi}(\mathbf{z})\|p_{\boldsymbol\theta}(\mathbf{z})\big)</script>.
 Also we have observed the data samples which we want to reconstruct and 
make them as probable as possible. So we should maximize 
<script type="math/tex">\mathbb{E}_{q_\boldsymbol{\phi}({\mathbf{z}})}
{\log p_{\boldsymbol\theta}(\mathbf{x}|\mathbf{z})}</script>. In other words, before observing the data we had some information and now after observing them we have further information about the model and our approximation for the posterior should make a balance between them.</p>

<!-- <figure>      
	<img src="/assets/latent_variable.png" alt="image"
	>
	<figcaption>
		latent variable
	</figcaption>
</figure> -->

	</div>
</article>

<div id="post-nav">
	<div class="next">
		
		<a href="/2018/12/28/EMalgorithm.html">
			<span>Next entry</span>
			EM Algorithm and Maximum Likelihood
		</a>
		
	</div>

	<div class="previous">
		
	</div>
</div>

		</div>
	</div>

	<footer class="site-footer">

	<div class="wrapper">

		<h2 class="footer-heading">Probabilistic Machine Learning â€¢ <a href="/table-of-contents">Table of Contents</a></h2>

		<div class="footer-col-wrapper">
			<div class="footer-col footer-col-1">
				<ul class="contact-list">
					<li>Probabilistic Machine Learning</li>
					<li><a href="mailto:rezaee1@umbc.edu">rezaee1@umbc.edu</a></li>
				</ul>
			</div>

			<div class="footer-col footer-col-2">
				<ul class="social-media-list">
					
					<li>
						<a href="https://github.com/jekyll"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jekyll</span></a>

					</li>
					

					
					<li>
						<a href="https://instagram.com/jekyllrb"><span class="icon icon--instagram"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#828282" d="M14.829 6.302c-.738-.034-.96-.04-2.829-.04s-2.09.007-2.828.04c-1.899.087-2.783.986-2.87 2.87-.033.738-.041.959-.041 2.828s.008 2.09.041 2.829c.087 1.879.967 2.783 2.87 2.87.737.033.959.041 2.828.041 1.87 0 2.091-.007 2.829-.041 1.899-.086 2.782-.988 2.87-2.87.033-.738.04-.96.04-2.829s-.007-2.09-.04-2.828c-.088-1.883-.973-2.783-2.87-2.87zm-2.829 9.293c-1.985 0-3.595-1.609-3.595-3.595 0-1.985 1.61-3.594 3.595-3.594s3.595 1.609 3.595 3.594c0 1.985-1.61 3.595-3.595 3.595zm3.737-6.491c-.464 0-.84-.376-.84-.84 0-.464.376-.84.84-.84.464 0 .84.376.84.84 0 .463-.376.84-.84.84zm-1.404 2.896c0 1.289-1.045 2.333-2.333 2.333s-2.333-1.044-2.333-2.333c0-1.289 1.045-2.333 2.333-2.333s2.333 1.044 2.333 2.333zm-2.333-12c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.958 14.886c-.115 2.545-1.532 3.955-4.071 4.072-.747.034-.986.042-2.887.042s-2.139-.008-2.886-.042c-2.544-.117-3.955-1.529-4.072-4.072-.034-.746-.042-.985-.042-2.886 0-1.901.008-2.139.042-2.886.117-2.544 1.529-3.955 4.072-4.071.747-.035.985-.043 2.886-.043s2.14.008 2.887.043c2.545.117 3.957 1.532 4.071 4.071.034.747.042.985.042 2.886 0 1.901-.008 2.14-.042 2.886z"/></svg>
</span><span class="username">jekyllrb</span></a>

					</li>
					

					
					<li>
						<a href="https://twitter.com/jekyllrb"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jekyllrb</span></a>

					</li>
					
				</ul>
			</div>

			<div class="footer-col footer-col-3">
				<p>Short tutorials and proofs in machine learning.
</p>
			</div>
		</div>

	</div>

</footer>


</body>

</html>
