<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="theme-color" content="#943526">

	
	<meta property="og:image" content="https://markovblanket.github.io/thumbnail.jpg">
	

	
	<meta property="og:title" content="Probabilistic Machine Learning • EM Algorithm and Maximum Likelihood">
	

	<link rel="icon" href="/favicon.png" type="image/png">

	<title>EM Algorithm and Maximum Likelihood</title>
	<meta name="description" content="In this post first we talk about the importance of maximum likelihood and then we evaluate the Expectation Maximization (EM) algorithm.">

	<link rel='stylesheet' id='libretto-fonts-css'  href='https://fonts.googleapis.com/css?family=Libre+Baskerville%3A400%2C700%2C400italic%7CPlayfair+Display%3A400%2C700%2C400italic%2C700italic%7CPlayfair+Display+SC%3A700%2C700italic%7CMontserrat%3A400%7CDroid+Sans+Mono%3A400&#038;subset=latin%2Clatin-ext' type='text/css' media='all'>

	<link rel="stylesheet" href="/css/main.css">
	<link rel="canonical" href="https://markovblanket.github.io/2018/12/28/EMalgorithm.html">
	<link rel="alternate" type="application/rss+xml" title="Probabilistic Machine Learning" href="https://markovblanket.github.io/feed.xml">

	<script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
	      jax: ["input/TeX", "output/HTML-CSS"],
	      tex2jax: {
	        inlineMath: [ ['$', '$'], ["\(", "\)"] ],
	        displayMath: [ ['$$', '$$'], ["\[", "\]"] ],
	        processEscapes: true,
	        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
	      }
	      //,
	      //displayAlign: "left",
	      //displayIndent: "2em"
	    });
	</script>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>



</head>


<body>

	<header class="site-header">

	<div class="wrapper">

		<a class="site-title" href="/">Probabilistic Machine Learning</a>

	</div>

</header>


	<div class="page-content">
		<div class="wrapper">
			<header class="post-header">
	<div>
		<span>Posted on </span><span class="post-meta">December 28, 2018</span>
	</div>

	<h1 class="post-title" itemprop="name headline">EM Algorithm and Maximum Likelihood</h1>
</header>

<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="entry-content" itemprop="articleBody">
		<p>In this post first we talk about the importance of maximum likelihood and then we evaluate 
the Expectation Maximization (EM<script type="math/tex"></script>) algorithm.</p>

<h2 id="maximum-likelihood-interpretation">Maximum Likelihood Interpretation:</h2>
<p>Suppose that we have a dataset of i.i.d samples <script type="math/tex">\{x_i\}_{i=1}^{N_D}</script> 
in which <script type="math/tex">N_D</script> is the number of datapoints. We are assuming each datapoint was independently sampled from <script type="math/tex">p(x;\boldsymbol\theta)</script> and we want to estimate <script type="math/tex">\boldsymbol\theta</script>. Since these samples are i.i.d we can factorize their joint probability:</p>

<script type="math/tex; mode=display">p(x_1,x_2,\ldots,x_{N_D};\boldsymbol\theta)
=
\prod_{i=1}^{N_D}
{p(x_i;\boldsymbol{\theta})}</script>

<p>Maximum likelihood method looks for the parameter that maximizes this joint probability (or the log of joint probability<script type="math/tex"></script>):</p>

<script type="math/tex; mode=display">\hat{\boldsymbol\theta}=\arg\max_{\boldsymbol\theta}
\sum_{i=1}^{N_D}{\log p(x_i;\boldsymbol\theta)}</script>

<p>Now suppose that <script type="math/tex">y=\{y_k\}_{k=1}^{K}</script> is the set of unique datapoints and <script type="math/tex">N_{D}=\sum_{k=1}^{K}{N_k}</script>, so we can rewrite the equation for <script type="math/tex">\hat{\boldsymbol\theta}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{\boldsymbol\theta}&=
\arg\max_{\boldsymbol\theta}
\sum_{k=1}^{K}{N_k\log p(y_k;\boldsymbol\theta)}\\

&=\arg\max_{\boldsymbol\theta}
\sum_{k=1}^{K}{\dfrac{N_k}{N_D}\log p(y_k;\boldsymbol\theta)}\\
\end{align} %]]></script>

<p>In this equation <script type="math/tex">\dfrac{N_k}{N_D}</script> is the frequency of each datapoint which shows the emprical distribution of unique datapoints, so by defining <script type="math/tex">q(y_k)</script> as our emprical distribution:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{\boldsymbol\theta}
&=\arg\max_{\boldsymbol\theta}
\sum_{k=1}^{K}{q(y_k)\log p(y_k;\boldsymbol\theta)}\\

&=\arg\max_{\boldsymbol\theta}
\sum_{k=1}^{K}{q(y_k)\log 
\dfrac{p(y_k;\boldsymbol\theta)}{q(y_k)}}\\

&=\arg\max_{\boldsymbol\theta}
D_{\mathrm{KL}}\big(q(y)\|p(y;\theta)\big)
\end{align} %]]></script>

<p>The last equation shows that by using maximum likelihood we are minimizing the KL-divergence between emprical distribution (<script type="math/tex">q(y)</script>) and the predefined model (<script type="math/tex">p(y;\theta)</script>).</p>

<h2 id="expectation-maximization">Expectation Maximization:</h2>
<p>Sometimes it’s not easy to find <script type="math/tex">\hat{\boldsymbol{\theta}}</script> directly, but adding a latent variable to the model 
makes the joint probability of <script type="math/tex">p(\boldsymbol{X},\mathbf{z};\boldsymbol{\theta})</script> easier to deal with. In this part we see how we can find an iterative algorithm to improve approximated parameters in each step. We know that for a concave function if <script type="math/tex">\lambda_1\geq 0,\ldots,\lambda_n\geq 0</script> and <script type="math/tex">\sum_{i} \lambda_i =1</script> we have Jensen’s inequality. 
<script type="math/tex">f\big(\sum_i \lambda_i x_i\big) \geq
\sum_i \lambda_i f(x_i)</script></p>

<p>Image below depicts this inequality for a simple case that we have just two points (<script type="math/tex">n=2</script>):</p>
<p align="center">
	<img width="100%" height="100%" src="/assets/concave3.png" alt="image" />
<!-- 	<figcaption>
		There is nothing on this earth more to be prized than true friendship.
	</figcaption>
 --></p>

<p>In statistics a special case of Jensen’s inequality holds for the expected value of a concave function.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathbb{E}[f(x)]&=\sum p(x)f(x)\\
&\leq
f\big(\sum xp(x)\big)\\
&=f\big(\mathbb{E}[x]\big)
 \end{align} %]]></script>

<p>Now suppose that <script type="math/tex">\mathbf{X}</script> is the set of all datapoints and in <script type="math/tex">n^{\mathrm{th}}</script> iteration the estimate for <script type="math/tex">\theta</script> is <script type="math/tex">\theta_n</script>. 
We want to maximize <script type="math/tex">L(\boldsymbol\theta)</script>:</p>

<script type="math/tex; mode=display">L(\boldsymbol\theta)=\log p(\mathbf{X};\boldsymbol\theta)</script>

<p>Since we want to make sure that in each iteration our likelihood is getting better (<script type="math/tex">L(\boldsymbol\theta_{n+1})\geq L(\boldsymbol\theta_{n}))</script>, 
our best action would be finding the parameter <script type="math/tex">\boldsymbol{\hat{\theta}}</script> that maximizes the distance between 
<script type="math/tex">L(\boldsymbol\theta)</script> and <script type="math/tex">L(\boldsymbol\theta_n)</script>:</p>

<script type="math/tex; mode=display">\Delta(\boldsymbol\theta|\boldsymbol\theta_n)=
L(\boldsymbol\theta)-
L(\boldsymbol\theta_n)= 
\log \bigg(\dfrac{p(\mathbf{X};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)
}
\bigg)</script>

<p>We know that <script type="math/tex">\log</script> function is concave and adding a sum inside the <script type="math/tex">\log</script> would provide this opportunity to use Jensen’s inequality. The numerator of the fraction inside log is a marginal probability that can be written as summation of joint probability:</p>

<script type="math/tex; mode=display">p(\mathbf{X};\boldsymbol\theta)=
	\sum_\mathbf{z}{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}</script>

<p>And we have:</p>

<script type="math/tex; mode=display">\Delta(\boldsymbol\theta|\boldsymbol\theta_n)=
\log \bigg(\sum_{\mathbf{z}}\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)
}
\bigg)</script>

<p>Now we see that <script type="math/tex">\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)
}</script>
is playing the role of <script type="math/tex">f(\mathbf{z})</script> and since the summation is over <script type="math/tex">\mathbf{z}</script>, we should add a probability distribution of <script type="math/tex">\mathbf{z}</script> (something like <script type="math/tex">p(\mathbf{z})</script>) to be able to use Jensen’s inequality. Our different options are:</p>

<p><strong>1-</strong> Our first option is <script type="math/tex">p(\mathbf{z};\boldsymbol\theta_n)</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\Delta(\boldsymbol\theta|\boldsymbol\theta_n)
&=\log \bigg(\sum_{\mathbf{z}}p(\mathbf{z};\boldsymbol\theta_n)
\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)p(\mathbf{z};\boldsymbol\theta_n)
}
\bigg)\\
&\geq 
\sum_{\mathbf{z}}p(\mathbf{z};\boldsymbol\theta_n)
\log
\bigg(
\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)p(\mathbf{z};\boldsymbol\theta_n)
}
\bigg)
\end{align} %]]></script>

<p>So we have found a lower bound for <script type="math/tex">\Delta(\boldsymbol\theta|\boldsymbol\theta_n)</script> 
and by maximizing this lower bound in each iteration we want to make sure that liklihood is getting better and better:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\boldsymbol{\theta}_{n+1}&=
\arg\max_{\theta}
\sum_{\mathbf{z}}p(\mathbf{z};\boldsymbol\theta_n)
\log
\bigg(
\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)p(\mathbf{z};\boldsymbol\theta_n)
}
\bigg)
\\
&=
\arg\max_{\theta}
\sum_{\mathbf{z}}p(\mathbf{z};\boldsymbol\theta_n)
\log
{
	p(\mathbf{X},\mathbf{z};\boldsymbol\theta)
}
\end{align} %]]></script>

<p>But <script type="math/tex">\boldsymbol\theta_n</script> 
is supposed to include all the parameters needed for the joint probability of <script type="math/tex">\boldsymbol{X}</script> and <script type="math/tex">\mathbf{z}</script>
which is 
<script type="math/tex">p(\mathbf{X},\mathbf{z};\boldsymbol{\theta_n})</script>
and in general case <script type="math/tex">p(\mathbf{z};\boldsymbol\theta_n)</script> is just using a subset of parameters defined in <script type="math/tex">\boldsymbol{\theta_n}</script> and some other parameters such as parameters needed for <script type="math/tex">p(\boldsymbol{X}|\mathbf{z})</script> may not be updated. So we can evaluate another option.</p>

<p><strong>2-</strong> The second option is 
<script type="math/tex">p(\mathbf{z}|\mathbf{X};\boldsymbol{\theta_n})</script> that uses all the parameters defined in <script type="math/tex">\boldsymbol{\theta_n}</script>:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\boldsymbol{\theta}_{n+1}&=
\arg\max_{\theta}
\sum_{\mathbf{z}}p(\mathbf{z}|\mathbf{X};\boldsymbol{\theta_n})
\log
\bigg(
\dfrac{p(\mathbf{X},\mathbf{z};\boldsymbol\theta)}
{
	p(\mathbf{X};\boldsymbol\theta_n)
	p(\mathbf{z}|\mathbf{X};\boldsymbol{\theta_n})
}
\bigg)
\\
&=
\arg\max_{\theta}
\sum_{\mathbf{z}}
p(\mathbf{z}|\mathbf{X};\boldsymbol{\theta_n})
\log
{
	p(\mathbf{X},\mathbf{z};\boldsymbol\theta)
}
\end{align} %]]></script>

<p>And this last equation is the general form of EM algorithm:</p>
<h1 id="e-step">E-step:</h1>
<p>Compute the expected value of joint probability:</p>

<script type="math/tex; mode=display">f(\boldsymbol\theta)=
\mathbb{E}_{p(\mathbf{z}|\mathbf{X};\boldsymbol{\theta_n})}
\bigg[
\log
{
	p(\mathbf{X},\mathbf{z};\boldsymbol\theta)
}
\bigg]</script>

<h1 id="m-step">M-step:</h1>
<p>Maximize <script type="math/tex">f(\boldsymbol\theta)</script> with respect to <script type="math/tex">\boldsymbol\theta</script>:</p>

<script type="math/tex; mode=display">\boldsymbol{\theta}_{n+1}=
\arg\max_{\boldsymbol\theta} 
f(\boldsymbol\theta)</script>

<p><!--First way--></p>
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_config = function () {
    // Here is an example,
    // this.page.url = "https://poanchen.github.io/2018/12/28/EMalgorithm.html";
    this.page.url = "https://markovblanket.github.io/2018/12/28/EMalgorithm.html";
    this.page.identifier = "/2018/12/28/EMalgorithm.html";
  };
  // You should be able to get the following lines of code from your Disqus admin.
  // https://disqus.com/admin/universalcode
  (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//markovblanket.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>

<noscript>
  Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>

<!--Second way-->
<div id="disqus_thread"></div>
<script src="/path/to/your/disqus.js"></script>

<noscript>
  Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>

<!-- it makes sence to 
Maximizing $$ L(\boldsymbol\theta) $$ in $${(n+1)}^{\mathrm{th}}$$ step is equivalent to maximizing $$L(\boldsymbol\theta)$$
 -->

	</div>
</article>

<div id="post-nav">
	<div class="next">
		
		<a href="/2018/12/28/sample-rejection.html">
			<span>Next entry</span>
			Rejection Sampling
		</a>
		
	</div>

	<div class="previous">
		
		<a href="/2018/12/28/variational-inference.html">
			<span>Previous entry</span>
			Variational Inference
		</a>
		
	</div>
</div>

		</div>
	</div>

	<footer class="site-footer">

	<div class="wrapper">

		<h2 class="footer-heading">Probabilistic Machine Learning • <a href="/table-of-contents">Table of Contents</a></h2>

		<div class="footer-col-wrapper">
			<div class="footer-col footer-col-1">
				<ul class="contact-list">
					<li>Probabilistic Machine Learning</li>
					<li><a href="mailto:rezaee1@umbc.edu">rezaee1@umbc.edu</a></li>
				</ul>
			</div>

			<div class="footer-col footer-col-2">
				<ul class="social-media-list">
					
					<li>
						<a href="https://github.com/jekyll"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jekyll</span></a>

					</li>
					

					
					<li>
						<a href="https://instagram.com/jekyllrb"><span class="icon icon--instagram"><svg width="24" height="24" viewBox="0 0 24 24"><path fill="#828282" d="M14.829 6.302c-.738-.034-.96-.04-2.829-.04s-2.09.007-2.828.04c-1.899.087-2.783.986-2.87 2.87-.033.738-.041.959-.041 2.828s.008 2.09.041 2.829c.087 1.879.967 2.783 2.87 2.87.737.033.959.041 2.828.041 1.87 0 2.091-.007 2.829-.041 1.899-.086 2.782-.988 2.87-2.87.033-.738.04-.96.04-2.829s-.007-2.09-.04-2.828c-.088-1.883-.973-2.783-2.87-2.87zm-2.829 9.293c-1.985 0-3.595-1.609-3.595-3.595 0-1.985 1.61-3.594 3.595-3.594s3.595 1.609 3.595 3.594c0 1.985-1.61 3.595-3.595 3.595zm3.737-6.491c-.464 0-.84-.376-.84-.84 0-.464.376-.84.84-.84.464 0 .84.376.84.84 0 .463-.376.84-.84.84zm-1.404 2.896c0 1.289-1.045 2.333-2.333 2.333s-2.333-1.044-2.333-2.333c0-1.289 1.045-2.333 2.333-2.333s2.333 1.044 2.333 2.333zm-2.333-12c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.958 14.886c-.115 2.545-1.532 3.955-4.071 4.072-.747.034-.986.042-2.887.042s-2.139-.008-2.886-.042c-2.544-.117-3.955-1.529-4.072-4.072-.034-.746-.042-.985-.042-2.886 0-1.901.008-2.139.042-2.886.117-2.544 1.529-3.955 4.072-4.071.747-.035.985-.043 2.886-.043s2.14.008 2.887.043c2.545.117 3.957 1.532 4.071 4.071.034.747.042.985.042 2.886 0 1.901-.008 2.14-.042 2.886z"/></svg>
</span><span class="username">jekyllrb</span></a>

					</li>
					

					
					<li>
						<a href="https://twitter.com/jekyllrb"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jekyllrb</span></a>

					</li>
					
				</ul>
			</div>

			<div class="footer-col footer-col-3">
				<p>Short tutorials and proofs in machine learning.
</p>
			</div>
		</div>

	</div>

</footer>


</body>

</html>
